From eec5631195a09feb373ea7db15910726e7108464 Mon Sep 17 00:00:00 2001
From: Jiangjin Wang <the@salted.fish>
Date: Wed, 29 Nov 2023 23:14:38 +0000
Subject: [PATCH 08/11] Bug 1855960 - [MIPS64] Make some assembler routines
 return FaultingCodeOffset. r=jseward

Differential Revision: https://phabricator.services.mozilla.com/D194834
---
 .../MacroAssembler-mips-shared-inl.h          |  13 +-
 .../MacroAssembler-mips-shared.cpp            | 118 ++++++-----
 .../mips-shared/MacroAssembler-mips-shared.h  |  28 +--
 js/src/jit/mips64/MacroAssembler-mips64.cpp   | 198 +++++++++++-------
 js/src/jit/mips64/MacroAssembler-mips64.h     |  74 +++----
 5 files changed, 252 insertions(+), 179 deletions(-)

diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h b/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
index 7a9fcf7c1b..efcef9314d 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
@@ -1283,25 +1283,20 @@ void MacroAssembler::spectreZeroRegister(Condition cond, Register scratch,
 
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedDouble(
     FloatRegister src, const Address& addr) {
-  // FIXME -- see https://bugzilla.mozilla.org/show_bug.cgi?id=1855960
-  return FaultingCodeOffset();
-  ma_sd(src, addr);
+  return ma_sd(src, addr);
 }
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedDouble(
     FloatRegister src, const BaseIndex& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_sd(src, addr);
+  return ma_sd(src, addr);
 }
 
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedFloat32(
     FloatRegister src, const Address& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_ss(src, addr);
+  return ma_ss(src, addr);
 }
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedFloat32(
     FloatRegister src, const BaseIndex& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_ss(src, addr);
+  return ma_ss(src, addr);
 }
 
 void MacroAssembler::memoryBarrier(MemoryBarrierBits barrier) {
diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp b/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
index 2463e4d69c..c8e4d78959 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
@@ -457,9 +457,10 @@ void MacroAssemblerMIPSShared::ma_mod_mask(Register src, Register dest,
 
 // Memory.
 
-void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
-                                       LoadStoreSize size,
-                                       LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_load(
+    Register dest, const BaseIndex& src, LoadStoreSize size,
+    LoadStoreExtension extension) {
+  FaultingCodeOffset fco;
   if (isLoongson() && ZeroExtend != extension &&
       Imm8::IsInSignedRange(src.offset)) {
     Register index = src.index;
@@ -476,6 +477,7 @@ void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
 #endif
     }
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gslbx(dest, src.base, index, src.offset);
@@ -492,12 +494,12 @@ void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
       default:
         MOZ_CRASH("Invalid argument for ma_load");
     }
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_load(dest, Address(SecondScratchReg, src.offset), size,
-                   extension);
+  return asMasm().ma_load(dest, Address(SecondScratchReg, src.offset), size,
+                          extension);
 }
 
 void MacroAssemblerMIPSShared::ma_load_unaligned(Register dest,
@@ -660,13 +662,15 @@ void MacroAssemblerMIPSShared::ma_load_unaligned(
       MOZ_CRASH("Invalid argument for ma_load");
   }
 
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
 }
 
-void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
-                                        LoadStoreSize size,
-                                        LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_store(
+    Register data, const BaseIndex& dest, LoadStoreSize size,
+    LoadStoreExtension extension) {
   if (isLoongson() && Imm8::IsInSignedRange(dest.offset)) {
+    FaultingCodeOffset fco;
     Register index = dest.index;
 
     if (dest.scale != TimesOne) {
@@ -681,6 +685,7 @@ void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
 #endif
     }
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gssbx(data, dest.base, index, dest.offset);
@@ -697,12 +702,12 @@ void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
       default:
         MOZ_CRASH("Invalid argument for ma_store");
     }
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(dest, SecondScratchReg);
-  asMasm().ma_store(data, Address(SecondScratchReg, dest.offset), size,
-                    extension);
+  return asMasm().ma_store(data, Address(SecondScratchReg, dest.offset), size,
+                           extension);
 }
 
 void MacroAssemblerMIPSShared::ma_store(Imm32 imm, const BaseIndex& dest,
@@ -890,7 +895,8 @@ void MacroAssemblerMIPSShared::ma_store_unaligned(
     default:
       MOZ_CRASH("Invalid argument for ma_store");
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 // Branches when done from within mips-specific code.
@@ -1360,7 +1366,8 @@ void MacroAssemblerMIPSShared::ma_lis(FloatRegister dest, float value) {
   }
 }
 
-void MacroAssemblerMIPSShared::ma_sd(FloatRegister ft, BaseIndex address) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_sd(FloatRegister ft,
+                                                   BaseIndex address) {
   if (isLoongson() && Imm8::IsInSignedRange(address.offset)) {
     Register index = address.index;
 
@@ -1376,15 +1383,17 @@ void MacroAssemblerMIPSShared::ma_sd(FloatRegister ft, BaseIndex address) {
 #endif
     }
 
+    FaultingCodeOffset fco = FaultingCodeOffset(currentOffset());
     as_gssdx(ft, address.base, index, address.offset);
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(address, SecondScratchReg);
-  asMasm().ma_sd(ft, Address(SecondScratchReg, address.offset));
+  return asMasm().ma_sd(ft, Address(SecondScratchReg, address.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ss(FloatRegister ft, BaseIndex address) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ss(FloatRegister ft,
+                                                   BaseIndex address) {
   if (isLoongson() && Imm8::IsInSignedRange(address.offset)) {
     Register index = address.index;
 
@@ -1400,22 +1409,25 @@ void MacroAssemblerMIPSShared::ma_ss(FloatRegister ft, BaseIndex address) {
 #endif
     }
 
+    FaultingCodeOffset fco = FaultingCodeOffset(currentOffset());
     as_gsssx(ft, address.base, index, address.offset);
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(address, SecondScratchReg);
-  asMasm().ma_ss(ft, Address(SecondScratchReg, address.offset));
+  return asMasm().ma_ss(ft, Address(SecondScratchReg, address.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ld(FloatRegister ft, const BaseIndex& src) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ld(FloatRegister ft,
+                                                   const BaseIndex& src) {
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_ld(ft, Address(SecondScratchReg, src.offset));
+  return asMasm().ma_ld(ft, Address(SecondScratchReg, src.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ls(FloatRegister ft, const BaseIndex& src) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ls(FloatRegister ft,
+                                                   const BaseIndex& src) {
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_ls(ft, Address(SecondScratchReg, src.offset));
+  return asMasm().ma_ls(ft, Address(SecondScratchReg, src.offset));
 }
 
 void MacroAssemblerMIPSShared::ma_bc1s(FloatRegister lhs, FloatRegister rhs,
@@ -1540,24 +1552,24 @@ void MacroAssemblerMIPSShared::minMaxFloat32(FloatRegister srcDest,
   bind(&done);
 }
 
-void MacroAssemblerMIPSShared::loadDouble(const Address& address,
-                                          FloatRegister dest) {
-  asMasm().ma_ld(dest, address);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadDouble(const Address& address,
+                                                        FloatRegister dest) {
+  return asMasm().ma_ld(dest, address);
 }
 
-void MacroAssemblerMIPSShared::loadDouble(const BaseIndex& src,
-                                          FloatRegister dest) {
-  asMasm().ma_ld(dest, src);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadDouble(const BaseIndex& src,
+                                                        FloatRegister dest) {
+  return asMasm().ma_ld(dest, src);
 }
 
-void MacroAssemblerMIPSShared::loadFloat32(const Address& address,
-                                           FloatRegister dest) {
-  asMasm().ma_ls(dest, address);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadFloat32(const Address& address,
+                                                         FloatRegister dest) {
+  return asMasm().ma_ls(dest, address);
 }
 
-void MacroAssemblerMIPSShared::loadFloat32(const BaseIndex& src,
-                                           FloatRegister dest) {
-  asMasm().ma_ls(dest, src);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadFloat32(const BaseIndex& src,
+                                                         FloatRegister dest) {
+  return asMasm().ma_ls(dest, src);
 }
 
 void MacroAssemblerMIPSShared::ma_call(ImmPtr dest) {
@@ -2113,6 +2125,9 @@ void MacroAssemblerMIPSShared::wasmLoadImpl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(access,
+                  wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+                  FaultingCodeOffset(currentOffset()));
   if (isFloat) {
     if (byteSize == 4) {
       asMasm().ma_ls(output.fpu(), address);
@@ -2124,7 +2139,6 @@ void MacroAssemblerMIPSShared::wasmLoadImpl(
                      static_cast<LoadStoreSize>(8 * byteSize),
                      isSigned ? SignExtend : ZeroExtend);
   }
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2163,6 +2177,10 @@ void MacroAssemblerMIPSShared::wasmStoreImpl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  // Only the last emitted instruction is a memory access.
+  asMasm().append(
+      access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+      FaultingCodeOffset(currentOffset()));
   if (isFloat) {
     if (byteSize == 4) {
       asMasm().ma_ss(value.fpu(), address);
@@ -2174,8 +2192,6 @@ void MacroAssemblerMIPSShared::wasmStoreImpl(
                       static_cast<LoadStoreSize>(8 * byteSize),
                       isSigned ? SignExtend : ZeroExtend);
   }
-  // Only the last emitted instruction is a memory access.
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2219,7 +2235,8 @@ static void CompareExchange(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2250,7 +2267,8 @@ static void CompareExchange(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2362,7 +2380,8 @@ static void AtomicExchange(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2400,7 +2419,8 @@ static void AtomicExchange(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(output, SecondScratchReg, 0);
@@ -2498,7 +2518,8 @@ static void AtomicFetchOp(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2547,7 +2568,8 @@ static void AtomicFetchOp(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2678,7 +2700,8 @@ static void AtomicEffectOp(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2727,7 +2750,8 @@ static void AtomicEffectOp(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared.h b/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
index 718903252c..137716bd3c 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
@@ -101,9 +101,9 @@ class MacroAssemblerMIPSShared : public Assembler {
   void ma_ctz(Register rd, Register rs);
 
   // load
-  void ma_load(Register dest, const BaseIndex& src,
-               LoadStoreSize size = SizeWord,
-               LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_load(Register dest, const BaseIndex& src,
+                             LoadStoreSize size = SizeWord,
+                             LoadStoreExtension extension = SignExtend);
   void ma_load_unaligned(Register dest, const BaseIndex& src,
                          LoadStoreSize size = SizeWord,
                          LoadStoreExtension extension = SignExtend);
@@ -115,9 +115,9 @@ class MacroAssemblerMIPSShared : public Assembler {
                          LoadStoreSize size, LoadStoreExtension extension);
 
   // store
-  void ma_store(Register data, const BaseIndex& dest,
-                LoadStoreSize size = SizeWord,
-                LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_store(Register data, const BaseIndex& dest,
+                              LoadStoreSize size = SizeWord,
+                              LoadStoreExtension extension = SignExtend);
   void ma_store(Imm32 imm, const BaseIndex& dest, LoadStoreSize size = SizeWord,
                 LoadStoreExtension extension = SignExtend);
   void ma_store_unaligned(Register data, const Address& dest,
@@ -183,11 +183,11 @@ class MacroAssemblerMIPSShared : public Assembler {
   // fp instructions
   void ma_lis(FloatRegister dest, float value);
 
-  void ma_sd(FloatRegister src, BaseIndex address);
-  void ma_ss(FloatRegister src, BaseIndex address);
+  FaultingCodeOffset ma_sd(FloatRegister src, BaseIndex address);
+  FaultingCodeOffset ma_ss(FloatRegister src, BaseIndex address);
 
-  void ma_ld(FloatRegister dest, const BaseIndex& src);
-  void ma_ls(FloatRegister dest, const BaseIndex& src);
+  FaultingCodeOffset ma_ld(FloatRegister dest, const BaseIndex& src);
+  FaultingCodeOffset ma_ls(FloatRegister dest, const BaseIndex& src);
 
   // FP branches
   void ma_bc1s(FloatRegister lhs, FloatRegister rhs, Label* label,
@@ -224,11 +224,11 @@ class MacroAssemblerMIPSShared : public Assembler {
   void minMaxFloat32(FloatRegister srcDest, FloatRegister other, bool handleNaN,
                      bool isMax);
 
-  void loadDouble(const Address& addr, FloatRegister dest);
-  void loadDouble(const BaseIndex& src, FloatRegister dest);
+  FaultingCodeOffset loadDouble(const Address& addr, FloatRegister dest);
+  FaultingCodeOffset loadDouble(const BaseIndex& src, FloatRegister dest);
 
-  void loadFloat32(const Address& addr, FloatRegister dest);
-  void loadFloat32(const BaseIndex& src, FloatRegister dest);
+  FaultingCodeOffset loadFloat32(const Address& addr, FloatRegister dest);
+  FaultingCodeOffset loadFloat32(const BaseIndex& src, FloatRegister dest);
 
   void outOfLineWasmTruncateToInt32Check(FloatRegister input, Register output,
                                          MIRType fromType, TruncFlags flags,
diff --git a/js/src/jit/mips64/MacroAssembler-mips64.cpp b/js/src/jit/mips64/MacroAssembler-mips64.cpp
index a6eab1b1fd..2930d2ea3e 100644
--- a/js/src/jit/mips64/MacroAssembler-mips64.cpp
+++ b/js/src/jit/mips64/MacroAssembler-mips64.cpp
@@ -598,17 +598,19 @@ void MacroAssemblerMIPS64::ma_mulPtrTestOverflow(Register rd, Register rs,
 }
 
 // Memory.
-void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
-                                   LoadStoreSize size,
-                                   LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_load(Register dest, Address address,
+                                                 LoadStoreSize size,
+                                                 LoadStoreExtension extension) {
   int16_t encodedOffset;
   Register base;
+  FaultingCodeOffset fco;
 
   if (isLoongson() && ZeroExtend != extension &&
       !Imm16::IsInSignedRange(address.offset)) {
     ma_li(ScratchRegister, Imm32(address.offset));
     base = address.base;
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gslbx(dest, base, ScratchRegister, 0);
@@ -625,7 +627,7 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
       default:
         MOZ_CRASH("Invalid argument for ma_load");
     }
-    return;
+    return fco;
   }
 
   if (!Imm16::IsInSignedRange(address.offset)) {
@@ -638,6 +640,7 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
     base = address.base;
   }
 
+  fco = FaultingCodeOffset(currentOffset());
   switch (size) {
     case SizeByte:
       if (ZeroExtend == extension) {
@@ -666,18 +669,21 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
     default:
       MOZ_CRASH("Invalid argument for ma_load");
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_store(Register data, Address address,
-                                    LoadStoreSize size,
-                                    LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_store(
+    Register data, Address address, LoadStoreSize size,
+    LoadStoreExtension extension) {
   int16_t encodedOffset;
   Register base;
+  FaultingCodeOffset fco;
 
   if (isLoongson() && !Imm16::IsInSignedRange(address.offset)) {
     ma_li(ScratchRegister, Imm32(address.offset));
     base = address.base;
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gssbx(data, base, ScratchRegister, 0);
@@ -694,7 +700,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
       default:
         MOZ_CRASH("Invalid argument for ma_store");
     }
-    return;
+    return fco;
   }
 
   if (!Imm16::IsInSignedRange(address.offset)) {
@@ -707,6 +713,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
     base = address.base;
   }
 
+  fco = FaultingCodeOffset(currentOffset());
   switch (size) {
     case SizeByte:
       as_sb(data, base, encodedOffset);
@@ -723,6 +730,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
     default:
       MOZ_CRASH("Invalid argument for ma_store");
   }
+  return fco;
 }
 
 void MacroAssemblerMIPS64Compat::computeScaledAddress(const BaseIndex& address,
@@ -977,64 +985,88 @@ void MacroAssemblerMIPS64::ma_mv(ValueOperand src, FloatRegister dest) {
   as_dmtc1(src.valueReg(), dest);
 }
 
-void MacroAssemblerMIPS64::ma_ls(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ls(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_lwc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gslsx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_lwc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_ld(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ld(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_ldc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gsldx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_ldc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_sd(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_sd(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_sdc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gssdx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_sdc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_ss(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ss(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_swc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gsssx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_swc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
 void MacroAssemblerMIPS64::ma_pop(FloatRegister f) {
@@ -1082,53 +1114,54 @@ void MacroAssemblerMIPS64Compat::movePtr(wasm::SymbolicAddress imm,
   ma_liPatchable(dest, ImmWord(-1));
 }
 
-void MacroAssemblerMIPS64Compat::load8ZeroExtend(const Address& address,
-                                                 Register dest) {
-  ma_load(dest, address, SizeByte, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8ZeroExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeByte, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8ZeroExtend(const BaseIndex& src,
-                                                 Register dest) {
-  ma_load(dest, src, SizeByte, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8ZeroExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeByte, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8SignExtend(const Address& address,
-                                                 Register dest) {
-  ma_load(dest, address, SizeByte, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8SignExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeByte, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8SignExtend(const BaseIndex& src,
-                                                 Register dest) {
-  ma_load(dest, src, SizeByte, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8SignExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeByte, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16ZeroExtend(const Address& address,
-                                                  Register dest) {
-  ma_load(dest, address, SizeHalfWord, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16ZeroExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeHalfWord, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16ZeroExtend(const BaseIndex& src,
-                                                  Register dest) {
-  ma_load(dest, src, SizeHalfWord, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16ZeroExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeHalfWord, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16SignExtend(const Address& address,
-                                                  Register dest) {
-  ma_load(dest, address, SizeHalfWord, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16SignExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeHalfWord, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16SignExtend(const BaseIndex& src,
-                                                  Register dest) {
-  ma_load(dest, src, SizeHalfWord, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16SignExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeHalfWord, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load32(const Address& address, Register dest) {
-  ma_load(dest, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load32(const Address& address,
+                                                      Register dest) {
+  return ma_load(dest, address, SizeWord);
 }
 
-void MacroAssemblerMIPS64Compat::load32(const BaseIndex& address,
-                                        Register dest) {
-  ma_load(dest, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load32(const BaseIndex& address,
+                                                      Register dest) {
+  return ma_load(dest, address, SizeWord);
 }
 
 void MacroAssemblerMIPS64Compat::load32(AbsoluteAddress address,
@@ -1143,13 +1176,14 @@ void MacroAssemblerMIPS64Compat::load32(wasm::SymbolicAddress address,
   load32(Address(ScratchRegister, 0), dest);
 }
 
-void MacroAssemblerMIPS64Compat::loadPtr(const Address& address,
-                                         Register dest) {
-  ma_load(dest, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::loadPtr(const Address& address,
+                                                       Register dest) {
+  return ma_load(dest, address, SizeDouble);
 }
 
-void MacroAssemblerMIPS64Compat::loadPtr(const BaseIndex& src, Register dest) {
-  ma_load(dest, src, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::loadPtr(const BaseIndex& src,
+                                                       Register dest) {
+  return ma_load(dest, src, SizeDouble);
 }
 
 void MacroAssemblerMIPS64Compat::loadPtr(AbsoluteAddress address,
@@ -1184,7 +1218,8 @@ void MacroAssemblerMIPS64Compat::loadUnalignedDouble(
     load = as_ldl(temp, ScratchRegister, 7);
     as_ldr(temp, ScratchRegister, 0);
   }
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
   moveToDouble(temp, dest);
 }
 
@@ -1203,7 +1238,8 @@ void MacroAssemblerMIPS64Compat::loadUnalignedFloat32(
     load = as_lwl(temp, ScratchRegister, 3);
     as_lwr(temp, ScratchRegister, 0);
   }
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
   moveToFloat32(temp, dest);
 }
 
@@ -1212,16 +1248,18 @@ void MacroAssemblerMIPS64Compat::store8(Imm32 imm, const Address& address) {
   ma_store(SecondScratchReg, address, SizeByte);
 }
 
-void MacroAssemblerMIPS64Compat::store8(Register src, const Address& address) {
-  ma_store(src, address, SizeByte);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store8(Register src,
+                                                      const Address& address) {
+  return ma_store(src, address, SizeByte);
 }
 
 void MacroAssemblerMIPS64Compat::store8(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeByte);
 }
 
-void MacroAssemblerMIPS64Compat::store8(Register src, const BaseIndex& dest) {
-  ma_store(src, dest, SizeByte);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store8(Register src,
+                                                      const BaseIndex& dest) {
+  return ma_store(src, dest, SizeByte);
 }
 
 void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const Address& address) {
@@ -1229,17 +1267,18 @@ void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const Address& address) {
   ma_store(SecondScratchReg, address, SizeHalfWord);
 }
 
-void MacroAssemblerMIPS64Compat::store16(Register src, const Address& address) {
-  ma_store(src, address, SizeHalfWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store16(Register src,
+                                                       const Address& address) {
+  return ma_store(src, address, SizeHalfWord);
 }
 
 void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeHalfWord);
 }
 
-void MacroAssemblerMIPS64Compat::store16(Register src,
-                                         const BaseIndex& address) {
-  ma_store(src, address, SizeHalfWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store16(
+    Register src, const BaseIndex& address) {
+  return ma_store(src, address, SizeHalfWord);
 }
 
 void MacroAssemblerMIPS64Compat::store32(Register src,
@@ -1248,8 +1287,9 @@ void MacroAssemblerMIPS64Compat::store32(Register src,
   store32(src, Address(ScratchRegister, 0));
 }
 
-void MacroAssemblerMIPS64Compat::store32(Register src, const Address& address) {
-  ma_store(src, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store32(Register src,
+                                                       const Address& address) {
+  return ma_store(src, address, SizeWord);
 }
 
 void MacroAssemblerMIPS64Compat::store32(Imm32 src, const Address& address) {
@@ -1261,8 +1301,9 @@ void MacroAssemblerMIPS64Compat::store32(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeWord);
 }
 
-void MacroAssemblerMIPS64Compat::store32(Register src, const BaseIndex& dest) {
-  ma_store(src, dest, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store32(Register src,
+                                                       const BaseIndex& dest) {
+  return ma_store(src, dest, SizeWord);
 }
 
 template <typename T>
@@ -1297,14 +1338,14 @@ template void MacroAssemblerMIPS64Compat::storePtr<Address>(ImmGCPtr imm,
 template void MacroAssemblerMIPS64Compat::storePtr<BaseIndex>(
     ImmGCPtr imm, BaseIndex address);
 
-void MacroAssemblerMIPS64Compat::storePtr(Register src,
-                                          const Address& address) {
-  ma_store(src, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::storePtr(
+    Register src, const Address& address) {
+  return ma_store(src, address, SizeDouble);
 }
 
-void MacroAssemblerMIPS64Compat::storePtr(Register src,
-                                          const BaseIndex& address) {
-  ma_store(src, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::storePtr(
+    Register src, const BaseIndex& address) {
+  return ma_store(src, address, SizeDouble);
 }
 
 void MacroAssemblerMIPS64Compat::storePtr(Register src, AbsoluteAddress dest) {
@@ -1328,7 +1369,8 @@ void MacroAssemblerMIPS64Compat::storeUnalignedFloat32(
     store = as_swl(temp, ScratchRegister, 3);
     as_swr(temp, ScratchRegister, 0);
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 void MacroAssemblerMIPS64Compat::storeUnalignedDouble(
@@ -1348,7 +1390,8 @@ void MacroAssemblerMIPS64Compat::storeUnalignedDouble(
     store = as_sdl(temp, ScratchRegister, 7);
     as_sdr(temp, ScratchRegister, 0);
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 void MacroAssembler::clampDoubleToUint8(FloatRegister input, Register output) {
@@ -2469,10 +2512,12 @@ void MacroAssemblerMIPS64Compat::wasmLoadI64Impl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(access,
+                  wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+                  FaultingCodeOffset(currentOffset()));
   asMasm().ma_load(output.reg, address,
                    static_cast<LoadStoreSize>(8 * byteSize),
                    isSigned ? SignExtend : ZeroExtend);
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2503,10 +2548,12 @@ void MacroAssemblerMIPS64Compat::wasmStoreI64Impl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(
+      access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+      FaultingCodeOffset(currentOffset()));
   asMasm().ma_store(value.reg, address,
                     static_cast<LoadStoreSize>(8 * byteSize),
                     isSigned ? SignExtend : ZeroExtend);
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2527,7 +2574,8 @@ static void CompareExchange64(MacroAssembler& masm,
   masm.bind(&tryAgain);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
   masm.as_lld(output.reg, SecondScratchReg, 0);
 
@@ -2587,7 +2635,8 @@ static void AtomicExchange64(MacroAssembler& masm,
   masm.bind(&tryAgain);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_lld(output.reg, SecondScratchReg, 0);
@@ -2645,7 +2694,8 @@ static void AtomicFetchOp64(MacroAssembler& masm,
 
   masm.bind(&tryAgain);
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_lld(output.reg, SecondScratchReg, 0);
diff --git a/js/src/jit/mips64/MacroAssembler-mips64.h b/js/src/jit/mips64/MacroAssembler-mips64.h
index 3b403af868..acb214d7e1 100644
--- a/js/src/jit/mips64/MacroAssembler-mips64.h
+++ b/js/src/jit/mips64/MacroAssembler-mips64.h
@@ -101,12 +101,14 @@ class MacroAssemblerMIPS64 : public MacroAssemblerMIPSShared {
   void ma_dctz(Register rd, Register rs);
 
   // load
-  void ma_load(Register dest, Address address, LoadStoreSize size = SizeWord,
-               LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_load(Register dest, Address address,
+                             LoadStoreSize size = SizeWord,
+                             LoadStoreExtension extension = SignExtend);
 
   // store
-  void ma_store(Register data, Address address, LoadStoreSize size = SizeWord,
-                LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_store(Register data, Address address,
+                              LoadStoreSize size = SizeWord,
+                              LoadStoreExtension extension = SignExtend);
 
   // arithmetic based ops
   // add
@@ -174,10 +176,10 @@ class MacroAssemblerMIPS64 : public MacroAssemblerMIPSShared {
   void ma_mv(FloatRegister src, ValueOperand dest);
   void ma_mv(ValueOperand src, FloatRegister dest);
 
-  void ma_ls(FloatRegister ft, Address address);
-  void ma_ld(FloatRegister ft, Address address);
-  void ma_sd(FloatRegister ft, Address address);
-  void ma_ss(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ls(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ld(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_sd(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ss(FloatRegister ft, Address address);
 
   void ma_pop(FloatRegister f);
   void ma_push(FloatRegister f);
@@ -668,30 +670,30 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
   void movePtr(wasm::SymbolicAddress imm, Register dest);
   void movePtr(ImmGCPtr imm, Register dest);
 
-  void load8SignExtend(const Address& address, Register dest);
-  void load8SignExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load8SignExtend(const Address& address, Register dest);
+  FaultingCodeOffset load8SignExtend(const BaseIndex& src, Register dest);
 
-  void load8ZeroExtend(const Address& address, Register dest);
-  void load8ZeroExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load8ZeroExtend(const Address& address, Register dest);
+  FaultingCodeOffset load8ZeroExtend(const BaseIndex& src, Register dest);
 
-  void load16SignExtend(const Address& address, Register dest);
-  void load16SignExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load16SignExtend(const Address& address, Register dest);
+  FaultingCodeOffset load16SignExtend(const BaseIndex& src, Register dest);
 
   template <typename S>
   void load16UnalignedSignExtend(const S& src, Register dest) {
     ma_load_unaligned(dest, src, SizeHalfWord, SignExtend);
   }
 
-  void load16ZeroExtend(const Address& address, Register dest);
-  void load16ZeroExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load16ZeroExtend(const Address& address, Register dest);
+  FaultingCodeOffset load16ZeroExtend(const BaseIndex& src, Register dest);
 
   template <typename S>
   void load16UnalignedZeroExtend(const S& src, Register dest) {
     ma_load_unaligned(dest, src, SizeHalfWord, ZeroExtend);
   }
 
-  void load32(const Address& address, Register dest);
-  void load32(const BaseIndex& address, Register dest);
+  FaultingCodeOffset load32(const Address& address, Register dest);
+  FaultingCodeOffset load32(const BaseIndex& address, Register dest);
   void load32(AbsoluteAddress address, Register dest);
   void load32(wasm::SymbolicAddress address, Register dest);
 
@@ -700,11 +702,11 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_load_unaligned(dest, src, SizeWord, SignExtend);
   }
 
-  void load64(const Address& address, Register64 dest) {
-    loadPtr(address, dest.reg);
+  FaultingCodeOffset load64(const Address& address, Register64 dest) {
+    return loadPtr(address, dest.reg);
   }
-  void load64(const BaseIndex& address, Register64 dest) {
-    loadPtr(address, dest.reg);
+  FaultingCodeOffset load64(const BaseIndex& address, Register64 dest) {
+    return loadPtr(address, dest.reg);
   }
 
   template <typename S>
@@ -712,8 +714,8 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_load_unaligned(dest.reg, src, SizeDouble, ZeroExtend);
   }
 
-  void loadPtr(const Address& address, Register dest);
-  void loadPtr(const BaseIndex& src, Register dest);
+  FaultingCodeOffset loadPtr(const Address& address, Register dest);
+  FaultingCodeOffset loadPtr(const BaseIndex& src, Register dest);
   void loadPtr(AbsoluteAddress address, Register dest);
   void loadPtr(wasm::SymbolicAddress address, Register dest);
 
@@ -726,14 +728,14 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
                             const BaseIndex& src, Register temp,
                             FloatRegister dest);
 
-  void store8(Register src, const Address& address);
+  FaultingCodeOffset store8(Register src, const Address& address);
+  FaultingCodeOffset store8(Register src, const BaseIndex& address);
   void store8(Imm32 imm, const Address& address);
-  void store8(Register src, const BaseIndex& address);
   void store8(Imm32 imm, const BaseIndex& address);
 
-  void store16(Register src, const Address& address);
+  FaultingCodeOffset store16(Register src, const Address& address);
+  FaultingCodeOffset store16(Register src, const BaseIndex& address);
   void store16(Imm32 imm, const Address& address);
-  void store16(Register src, const BaseIndex& address);
   void store16(Imm32 imm, const BaseIndex& address);
 
   template <typename T>
@@ -741,9 +743,9 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_store_unaligned(src, dest, SizeHalfWord);
   }
 
+  FaultingCodeOffset store32(Register src, const Address& address);
+  FaultingCodeOffset store32(Register src, const BaseIndex& address);
   void store32(Register src, AbsoluteAddress address);
-  void store32(Register src, const Address& address);
-  void store32(Register src, const BaseIndex& address);
   void store32(Imm32 src, const Address& address);
   void store32(Imm32 src, const BaseIndex& address);
 
@@ -759,9 +761,11 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     storePtr(ImmWord(imm.value), address);
   }
 
-  void store64(Register64 src, Address address) { storePtr(src.reg, address); }
-  void store64(Register64 src, const BaseIndex& address) {
-    storePtr(src.reg, address);
+  FaultingCodeOffset store64(Register64 src, Address address) {
+    return storePtr(src.reg, address);
+  }
+  FaultingCodeOffset store64(Register64 src, const BaseIndex& address) {
+    return storePtr(src.reg, address);
   }
 
   template <typename T>
@@ -775,8 +779,8 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
   void storePtr(ImmPtr imm, T address);
   template <typename T>
   void storePtr(ImmGCPtr imm, T address);
-  void storePtr(Register src, const Address& address);
-  void storePtr(Register src, const BaseIndex& address);
+  FaultingCodeOffset storePtr(Register src, const Address& address);
+  FaultingCodeOffset storePtr(Register src, const BaseIndex& address);
   void storePtr(Register src, AbsoluteAddress dest);
 
   void storeUnalignedFloat32(const wasm::MemoryAccessDesc& access,
-- 
2.46.0

